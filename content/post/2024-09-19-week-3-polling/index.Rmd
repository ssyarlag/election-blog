---
title: Week 3 - Polling
author: Shriya Yarlagadda
date: '2024-09-19'
slug: week-3-polling
categories: []
tags: []
---

In this week's post, we focus on analyzing polling data, attempting to understand how they can best predict election outcomes. My thanks again to Matthew Dardet for his generous starter code.

For the purpose of this analysis, we focus on polling data from FiveThirtyEight. We began by visualizing the available data from the past several elections, understanding how polling data changes relative to the actual election outcome.

```{r echo = FALSE, message = FALSE, warning = FALSE}
# Load libraries and initial dataset
library(tidyverse)
library(modelsummary)
library(glmnet)

nationalpoll_av <- read_csv("data/national_polls_1968-2024.csv")
popvote_data <- read_csv("data/popvote_1948-2020.csv")
popvote_data$party[popvote_data$party == "democrat"] <- "DEM"
popvote_data$party[popvote_data$party == "republican"] <- "REP"

# Combine poll and actual data for each year from 1968-2020
poll_and_actual_data <- nationalpoll_av |>
  left_join(popvote_data |>
              select(-candidate),
            by = c("year", "party")) |>
  rename(final_poll = poll_support)
```

```{r echo = FALSE, message = FALSE, warning = FALSE}
# Plot recent presidential polling data over time, with dashed horizontal lines representing actual outcomes - reference (9)

polls_by_year <- poll_and_actual_data |>
  filter(year < 2024 & year > 1999) |>
  ggplot(mapping = aes(x = weeks_left, y =  final_poll, color = party)) +
  geom_line() +
  scale_color_manual(values = c("dodgerblue4", "firebrick1")) +
  scale_x_reverse() +
  geom_hline(mapping = aes(yintercept = pv2p, color = party), linetype = "dotdash", legend = "Election Results") + 
  labs(title = "Polling Data across Weeks vs. Election Results",
       x = "Weeks Left til Election",
       y = "Support for Candidate") +
  theme_minimal() +
  facet_wrap(~year)

polls_by_year
```

This data shows the polling averages at various stages of the election, benchmarked against the actual vote share earned by each candidate during that year's election (horizontal lines). This is very likely affected by events occurring during the election cycle and each campaign's response to them. For example, we see a substantial decline in support for Donald Trump around June 2020 - perhaps corresponding to the murder of George Floyd (1).

However, it is interesting to see if election forecasts made using polling data in the few weeks prior to the election are necessarily more predictive of the final outcomes than those considering prior weeks. Some, such Tien and Lewis-Beck, claim that "short-view forecasts," which solely rely on polling data, especially in the weeks prior to the election, tend to be inaccurate (2). While some of these charts above show relatively accurate November predictions (eg: Biden's polling in 2020), many others are very inaccurate. For example, polling about 15 weeks before the election better predicted Trump's 2016 vote share than polling in the weeks immediately prior. 

To test the accuracy of November polls, we computed several regressions of November polling averages on actual election outcomes between 1968 and 2020 (we have data as early as 1948, but we drop earlier years given that we do not have polling data from FiveThirtyEight from 1948-1964). I calculated three separate regressions, highlighted in the table below, which model predictions for only Democratic candidates, only Republicans, and both parties respectively.

```{r echo = FALSE, message = FALSE, warning = FALSE}

# Regression for only democratic candidates

ols.dem.nov <- lm(pv2p~final_poll,
                  data = subset (poll_and_actual_data, party == "DEM"))

# Regression for only republican candidates
ols.rep.nov <- lm(pv2p~final_poll,
                  data = subset (poll_and_actual_data, party == "REP"))

# Regression for all candidates
ols.all.nov <- lm(pv2p~final_poll,
                  data = poll_and_actual_data)


modelsummary(models = list(
  "Democrats Only" = ols.dem.nov,
  "Republicans Only" = ols.rep.nov,
  "All Candidates" = ols.all.nov), output = "novpolls.jpg")
```
![](images/novpolls.jpg)

Given that the Republican only model has an R^2 value of 0.727 while the Democrat only model has an R^2 value of 0.665 (with the combined model being in between), it appears that polling data from FiveThirtyEight has, on average, been more predictive of Republican outcomes. However, I also wanted to see if the relationship may have been skewed by earlier polling data, which was likely less accurate. To explore this, I wanted to see if earlier polls tended to be more inaccurate than more recent ones, starting with a scatterplot. 

```{r echo = FALSE, message = FALSE, warning = FALSE}

poll_and_actual_data |>
  select(year, pv2p, final_poll, party, poll_date) |>
  group_by(year) |>
  top_n(1, poll_date) |>
  ggplot(mapping = aes(x = final_poll, y = pv2p, color = party)) +
  geom_point(position = "dodge") +
  geom_label(aes(label = c(year))) + 
  scale_color_manual(values = c("dodgerblue4", "firebrick1")) + 
  labs(x = "November Poll Prediction",
       y = "Actual Vote Share",
       title = "Actual Vote Share vs November Poll Predictions by Year and Party") +
  geom_abline(slope = 1, intercept = 0) + # Referenced source 4
  theme_bw()
```

This scatterplot, which includes a line demonstrating perfect poll prediction, shows that most November polls are somewhat inaccurate, often underpredicting candidate performance. However, given that it didn't clearly demonstrate trends in performance over the years, I calculated the years/candidates with the lowest November prediction error, including them in the table below.


```{r echo = FALSE, message = FALSE, warning = FALSE}
poll_and_actual_data$pred_error = poll_and_actual_data$final_poll - poll_and_actual_data$pv2p

top10_polls <- poll_and_actual_data |>
  slice_min(abs(pred_error), n = 10) |>
  select(year, party, candidate, pv2p, final_poll, pred_error)

knitr::kable(top10_polls, digits = 4, col.names = c("Year", "Party", "Candidate", "Two-Party Vote Share", "Final Poll Prediction of Vote Share", "Prediction Error"), align = 'c')
```


This shows two interesting conclusions. First, all of the closest predictions have underpredicted candidate performance, showing a similar conclusion to the scatterplot. But second, and more relevantly, this tentatively suggests that recent polls haven't necessarily been more successful. Of course, we cannot make a causal argument from just this data and many recent years make up our top 10, polling from 1968, the earliest year in our dataset, makes our top 10. This may lend credence to the argument that there are certain "fundamentals" that can help predict voter performance, regardless of data accuracy and statistical/computing power that pollsters may have access to (2). 

Keeping the relative inaccuracy of only using November polling to predict outcomes in mind, we also wanted to explore if including polls at other times in an election cycle can result in better prediction outcomes. To do so, we used the regularization method of Elastic Net, identifying the most predictive weeks based on historical polling and election outcome data.

I trained this model using data from before 2020, evaluating it on the data from 2020. I did this because although we do have some data from 2024 and could reasonably make a prediction of this year's election using our polling-based model, the fact that the Democratic candidate changed midway through the year may reduce the likelihood of our model (which is largely trained on elections that did have a similar shift).

```{r echo = FALSE, message = FALSE, warning = FALSE}

# Find polling average by weeks left and merge with voting data
poll_weeks <- nationalpoll_av |>
  group_by(year, party, weeks_left) |>
  summarize(mean_poll_week = mean(poll_support)) |> 
  filter(weeks_left <= 30) |> 
  pivot_wider(names_from = weeks_left, values_from = mean_poll_week) |> 
  left_join(popvote_data, by = c("year", "party"))

# Add more descriptive column names to polling data columns
colnames(poll_weeks)[3:33] <- paste0("poll_weeks_left_", 0:30)

# Find training and test data
train_poll_weeks <- poll_weeks |>
  filter(year < 2020)

x.train <- train_poll_weeks |>
  ungroup() |>
  select(all_of(starts_with("poll_weeks_left"))) |>
  as.matrix()

y.train <- train_poll_weeks$pv2p


test_poll_weeks <- poll_weeks |>
  filter(year == 2020)

x.test <- test_poll_weeks |>
  ungroup() |>
  select(all_of(starts_with("poll_weeks_left"))) |>
  as.matrix()

# Create cross-validated prediction equation
set.seed(91534827)
cv.enet.pollweeks <- cv.glmnet(x=x.train, y = y.train, alpha = 0.5)

# Find minimum lambda value
lambda.min.enet <- cv.enet.pollweeks$lambda.min

# Predict on 2020 data
pred_2020 <- predict(cv.enet.pollweeks, s = lambda.min.enet, newx = x.test)

outcome_2020 <- poll_and_actual_data |>
  filter(year == 2020) |>
  select(year, party, pv2p)

dem_accuracy_2020 <- pred_2020[1] - outcome_2020$pv2p[1]
rep_accuracy_2020 <- pred_2020[2] - outcome_2020$pv2p[2]

# Referenced source 7 and 8 

accuracy_data = matrix(c(1:2), ncol = 2, byrow = TRUE)

colnames(accuracy_data) = c('Prediction Error - Democratic Party (2020)', 'Prediction Error - Republican Party (2020)')

accuracy_table = as_tibble(accuracy_data)

accuracy_table$`Prediction Error - Democratic Party (2020)` = dem_accuracy_2020

accuracy_table$`Prediction Error - Republican Party (2020)` = rep_accuracy_2020

knitr::kable(accuracy_table, align = 'c')

```


Our model predicted that, based on polling trends from previous years, Biden would earn 54.4 % of the two-party popular vote, while Trump would earn 48.3% in 2020. While these predictions were slightly high for both candidates, the poll was off by over 2 percentage points for Biden. On one hand, this may be due to the substantial variations between this election and previous ones. However, it may lend further credence to the earlier point that polls tend to overpredict candidate performance. Furthermore, it aligns with our earlier finding (using only November polling) that election forecasts tend to more accurately predict outcomes for Republican candidates.

Given these findings, it is interesting to compare the perspectives that two major pollsters involved with FiveThirtyEight -- Nate Silver and G. Elliot Morris -- have proposed regarding election forecasting, an *extension* of our previous analytical work.

In a June 2024 article (5), Silver highlights the changes in his 2024 prediction relative to his 2020 predictions. In this, he seems to focus on making minor tweaks to his previous model, saying that he "tend[s] to be conservative about making changes to the election models. He appears to focus on very broad factors that affect election outcomes like party level turnout and incumbency. However, in an article released around the same time (6), Morris encourages considering the fundamentals, but takes a different perspective, considering, for example "the rates of change of these indicators." In doing so, he also delves into greater specificity than Silver, observing characteristics like state effects on neighbors, seeming to stray away from a strict focus on "fundamentals."

# Citations

1. Silverstein, Jason. “The Global Impact of George Floyd: How Black Lives Matter Protests Shaped Movements around the World - CBS News,” June 4, 2021. https://www.cbsnews.com/news/george-floyd-black-lives-matter-impact/.

2. Tien, Charles, and Michael Lewis-Beck. “Evaluating the Long-View Forecasting Models of the 2016 Election.” OUPblog, January 11, 2017. https://blog.oup.com/2017/01/forecasting-models-2016-election/.

3. chemdork123. “Answer to ‘Change Scale on X Axis in Ggplot in R.’” Stack Overflow, July 20, 2020. https://stackoverflow.com/a/63003636.

4. Posit Community. “Add X=y Geom_abline to Ggplot - General,” June 1, 2019. https://forum.posit.co/t/add-x-y-geom-abline-to-ggplot/32154.

5. Silver, Nate. “2024 Presidential Election Model Methodology Update,” June 26, 2024. https://www.natesilver.net/p/model-methodology-2024.

6. Morris, G. Elliott. “How 538’s 2024 Presidential Election Forecast Works.” ABC News, June 11, 2024. https://abcnews.go.com/538/538s-2024-presidential-election-forecast-works/story?id=110867585.

7. GeeksforGeeks. “How to Create Tables in R?,” December 16, 2021. https://www.geeksforgeeks.org/how-to-create-tables-in-r/.

8. “Build a Data Frame — Tibble.” Accessed September 22, 2024. https://tibble.tidyverse.org/reference/tibble.html.

9. Daniel. “Answer to ‘Reverse the Scale of the x Axis in a Plot.’” Stack Overflow, March 28, 2019. https://stackoverflow.com/a/55390320.

Help documents for Model Summary, Top N, Slice_Max, Abs (Absolute Value), Geom_vline, Left/Right/Full Join, Linetype, kbl, Summarize

# Data Sources

Poll Predictions from FiveThirtyEight and National Election Data (both provided by course)

